// ARFoundationDepthProcessor.compute
// Optimized depth processing for iOS with Metal support
// Handles both human segmentation and LiDAR environment depth

#pragma kernel DepthToWorld
#pragma kernel ProcessVelocity
#pragma kernel GenerateNormals

// Constants
static const float EPSILON = 0.0001f;
static const int THREAD_GROUP_SIZE = 8;

// Input Textures
Texture2D<float> _HumanStencil;
Texture2D<float> _HumanDepth;
Texture2D<float> _EnvironmentDepth;
Texture2D<float4> _CameraTexture;

// Input for velocity/normal calculation
Texture2D<float4> _PositionInput;

// Output Textures
RWTexture2D<float4> _PositionOutput;
RWTexture2D<float4> _ColorOutput;
RWTexture2D<float4> _VelocityOutput;
RWTexture2D<float4> _NormalOutput;

// Previous frame data
Texture2D<float4> _PreviousPosition;
RWTexture2D<float4> _CurrentPosition; // Copy for next frame

// Matrices buffer
// [0] = Inverse VP Matrix
// [1] = Camera to World Matrix
// [2] = Previous VP Matrix
StructuredBuffer<float4x4> _Matrices;

// Parameters
float _DepthScale;
float _NearPlane;
float _FarPlane;
float _DeltaTime;
bool _HasHumanData;
bool _HasEnvironmentData;

// Samplers
SamplerState sampler_LinearClamp;
SamplerState sampler_PointClamp;

// Helper Functions
float3 DepthToWorldPosition(float2 uv, float depth, float4x4 invVPMatrix)
{
    // Convert UV to NDC (Normalized Device Coordinates)
    float2 ndc = uv * 2.0 - 1.0;
    ndc.y = -ndc.y; // Flip Y for Unity's coordinate system
    
    // Create clip space position
    float4 clipPos = float4(ndc.x, ndc.y, depth * 2.0 - 1.0, 1.0);
    
    // Transform to world space
    float4 worldPos = mul(invVPMatrix, clipPos);
    worldPos /= worldPos.w;
    
    return worldPos.xyz;
}

float LinearizeDepth(float depth)
{
    // Convert from non-linear depth buffer value to linear depth
    float z = depth * 2.0 - 1.0;
    return (2.0 * _NearPlane * _FarPlane) / (_FarPlane + _NearPlane - z * (_FarPlane - _NearPlane));
}

float3 GetCameraSpacePosition(float2 uv, float depth, float4x4 invProjection)
{
    float2 ndc = uv * 2.0 - 1.0;
    ndc.y = -ndc.y;
    
    float4 clipPos = float4(ndc.x, ndc.y, depth * 2.0 - 1.0, 1.0);
    float4 viewPos = mul(invProjection, clipPos);
    viewPos /= viewPos.w;
    
    return viewPos.xyz;
}

// Main Kernels
[numthreads(THREAD_GROUP_SIZE, THREAD_GROUP_SIZE, 1)]
void DepthToWorld(uint3 id : SV_DispatchThreadID)
{
    // Get output dimensions
    uint width, height;
    _PositionOutput.GetDimensions(width, height);
    
    // Bounds check
    if (id.x >= width || id.y >= height)
        return;
    
    // Calculate UV coordinates
    float2 uv = (float2(id.xy) + 0.5) / float2(width, height);
    
    // Initialize outputs
    float4 position = float4(0, 0, 0, 0);
    float4 color = float4(0, 0, 0, 0);
    
    // Get matrices
    float4x4 invVPMatrix = _Matrices[0];
    float4x4 cameraToWorld = _Matrices[1];
    
    // Process human depth if available
    if (_HasHumanData)
    {
        float stencil = _HumanStencil.SampleLevel(sampler_PointClamp, uv, 0).r;
        
        if (stencil > 0.5)
        {
            float humanDepth = _HumanDepth.SampleLevel(sampler_LinearClamp, uv, 0).r;
            
            // Apply depth scale
            humanDepth *= _DepthScale;
            
            // Convert to world position
            float3 worldPos = DepthToWorldPosition(uv, humanDepth, invVPMatrix);
            
            // Calculate distance from camera
            float3 cameraPos = float3(cameraToWorld[0][3], cameraToWorld[1][3], cameraToWorld[2][3]);
            float distance = length(worldPos - cameraPos);
            
            // Store position with confidence in alpha
            position = float4(worldPos, 1.0);
            
            // Sample color from camera texture
            if (_CameraTexture.SampleLevel(sampler_LinearClamp, uv, 0).a > 0)
            {
                color = _CameraTexture.SampleLevel(sampler_LinearClamp, uv, 0);
                color.a = 1.0; // Mark as human
            }
        }
    }
    
    // Fall back to environment depth if no human detected
    if (position.w < 0.5 && _HasEnvironmentData)
    {
        float envDepth = _EnvironmentDepth.SampleLevel(sampler_LinearClamp, uv, 0).r;
        
        if (envDepth > 0.0 && envDepth < 1.0)
        {
            // Convert to world position
            float3 worldPos = DepthToWorldPosition(uv, envDepth, invVPMatrix);
            
            // Store position with lower confidence
            position = float4(worldPos, 0.5);
            
            // Sample color
            if (_CameraTexture.SampleLevel(sampler_LinearClamp, uv, 0).a > 0)
            {
                color = _CameraTexture.SampleLevel(sampler_LinearClamp, uv, 0);
                color.a = 0.5; // Mark as environment
            }
        }
    }
    
    // Write outputs
    _PositionOutput[id.xy] = position;
    _ColorOutput[id.xy] = color;
    
    // Store for velocity calculation next frame
    _CurrentPosition[id.xy] = position;
}

[numthreads(THREAD_GROUP_SIZE, THREAD_GROUP_SIZE, 1)]
void ProcessVelocity(uint3 id : SV_DispatchThreadID)
{
    uint width, height;
    _VelocityOutput.GetDimensions(width, height);
    
    if (id.x >= width || id.y >= height)
        return;
    
    // Get current position
    float4 currentPos = _PositionInput[id.xy];
    
    // Initialize velocity
    float4 velocity = float4(0, 0, 0, 0);
    
    if (currentPos.w > 0.0)
    {
        // Get previous position
        float4 previousPos = _PreviousPosition[id.xy];
        
        if (previousPos.w > 0.0)
        {
            // Calculate velocity
            float3 vel = (currentPos.xyz - previousPos.xyz) / max(_DeltaTime, 0.001);
            
            // Clamp to reasonable values
            float speed = length(vel);
            if (speed > 10.0) // Max 10 m/s
            {
                vel = normalize(vel) * 10.0;
                speed = 10.0;
            }
            
            velocity = float4(vel, speed);
        }
    }
    
    _VelocityOutput[id.xy] = velocity;
}

[numthreads(THREAD_GROUP_SIZE, THREAD_GROUP_SIZE, 1)]
void GenerateNormals(uint3 id : SV_DispatchThreadID)
{
    uint width, height;
    _NormalOutput.GetDimensions(width, height);
    
    if (id.x >= width || id.y >= height)
        return;
    
    // Sample neighboring positions
    float4 center = _PositionInput[id.xy];
    
    if (center.w < 0.5)
    {
        _NormalOutput[id.xy] = float4(0, 0, 0, 0);
        return;
    }
    
    // Get neighboring pixels (cross pattern)
    int2 coordL = int2(max(0, id.x - 1), id.y);
    int2 coordR = int2(min(width - 1, id.x + 1), id.y);
    int2 coordU = int2(id.x, max(0, id.y - 1));
    int2 coordD = int2(id.x, min(height - 1, id.y + 1));
    
    float4 left = _PositionInput[coordL];
    float4 right = _PositionInput[coordR];
    float4 up = _PositionInput[coordU];
    float4 down = _PositionInput[coordD];
    
    // Calculate gradients
    float3 dx = float3(0, 0, 0);
    float3 dy = float3(0, 0, 0);
    
    if (left.w > 0.5 && right.w > 0.5)
    {
        dx = right.xyz - left.xyz;
    }
    else if (left.w > 0.5)
    {
        dx = center.xyz - left.xyz;
    }
    else if (right.w > 0.5)
    {
        dx = right.xyz - center.xyz;
    }
    
    if (up.w > 0.5 && down.w > 0.5)
    {
        dy = down.xyz - up.xyz;
    }
    else if (up.w > 0.5)
    {
        dy = center.xyz - up.xyz;
    }
    else if (down.w > 0.5)
    {
        dy = down.xyz - center.xyz;
    }
    
    // Calculate normal
    float3 normal = normalize(cross(dx, dy));
    
    // Flip if facing away from camera
    float4x4 cameraToWorld = _Matrices[1];
    float3 cameraPos = float3(cameraToWorld[0][3], cameraToWorld[1][3], cameraToWorld[2][3]);
    float3 viewDir = normalize(cameraPos - center.xyz);
    
    if (dot(normal, viewDir) < 0)
    {
        normal = -normal;
    }
    
    // Store normal with confidence based on neighbor availability
    float confidence = 0.25;
    if (left.w > 0.5) confidence += 0.25;
    if (right.w > 0.5) confidence += 0.25;
    if (up.w > 0.5) confidence += 0.25;
    if (down.w > 0.5) confidence += 0.25;
    
    _NormalOutput[id.xy] = float4(normal * 0.5 + 0.5, confidence);
}