# Feature Specification: WarpJobs Intelligence Engine

**Feature Branch**: `001-warpjobs-engine`
**Created**: 2025-12-06
**Status**: Draft
**Input**: User description: "I want to build the WarpJobs Intelligence Engine. It needs a scalable scraper (Node.js/Puppeteer preferred for speed) to fetch job data from major sites. It must then analyze this data using an LLM (likely Gemini) to generate 'Tony Stark' style actionable insights, priority scores, and specific tags based on my profile (XR, AI, Spatial Computing). The frontend should be a 'Bloomberg Terminal' style dashboard with real-time filtering, robust error handling, and 'zombie process' killing capabilities. It must be self-healing and local-first."

## User Scenarios & Testing *(mandatory)*

### User Story 1 - The Tony Stark Morning Routine (Priority: P1)
As a power user, I want to open my dashboard in the morning and immediately see a prioritized list of high-value job opportunities filtered by "XR", "AI", and "Spatial Computing", so that I don't waste time scrolling through irrelevant listings.

**Why this priority**: Core value proposition. "Cutting through the noise."

**Independent Test**:
1. Run scraper.
2. Open Dashboard.
3. Verify top 3 jobs are highly relevant (e.g., "Meta - AR Engine Dev", NOT "General IT Support").
4. Verify "Insights" column explains *why* it's relevant (e.g., "Direct fit for your HoloKit experience").

**Acceptance Scenarios**:
1. **Given** scraper has run, **When** I open the dashboard, **Then** I see a sorted table of jobs with a "Priority Score" (0-100).
2. **Given** a job description, **When** I hover over the "Insight" icon, **Then** I see a summary generated by the LLM (e.g., "Matches 8/10 keywords: Unity, C#, ARKit...").

---

### User Story 2 - Automated Scalable Intel Gathering (Priority: P1)
As a developer, I want the system to scrape multiple job sites (LinkedIn, Google Careers, YC) using a scalable Node.js crawler, handling pagination and anti-bot measures, so that my dataset is comprehensive.

**Why this priority**: Data volume and quality are essential for the intelligence engine to work.

**Independent Test**:
1. Trigger scraper command.
2. Verify logs show successful fetches from multiple domains.
3. Verify `jobs_data.json` increases in size and contains new non-duplicate entries.

**Acceptance Scenarios**:
1. **Given** the scraper is triggered, **When** it encounters a paginated list, **Then** it iterates through at least 5 pages.
2. **Given** a network error or "zombie process" from a previous run, **When** the script starts, **Then** it automatically kills old processes and retries.

---

### User Story 3 - Visual "Bloomberg" Analytics (Priority: P2)
As a user, I want to see real-time visualizations (sparklines, distribution graphs) of job trends (e.g., "XR jobs vs AI jobs this week"), so that I can spot market shifts.

**Why this priority**: Enhances the "Premium/Tony Stark" feel and provides macro-level insights.

**Independent Test**:
1. Open Dashboard.
2. Verify "Market Pulse" section shows graphs derived from `jobs_data.json`.

**Acceptance Scenarios**:
1. **Given** the dashboard is open, **When** data updates, **Then** the charts update without a full page reload.

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST support a **Node.js based scraper** (using Puppeteer or Playwright) for robust, headless browsing.
- **FR-002**: System MUST integrate with a Local or Cloud LLM (Gemini API or local model) to generate:
    - `priority_score`: Integer (0-100)
    - `match_explanation`: String (1-2 sentences)
    - `tags`: List[String] (extracted topics like "NeRF", "Gaussian Splatting")
- **FR-003**: Dashboard MUST be a single HTML file (or minimal set) served locally, with NO external build steps required for viewing (Vanilla JS/CSS).
- **FR-004**: System MUST implement "Self-Healing" start scripts that kill zombie processes on port 7777 (Python server) or hanging Node processes.
- **FR-005**: Data MUST be stored in `jobs_data.json` (or SQLite) locally. No external database dependency.
- **FR-006**: User Configuration MUST allow setting "Priority Keywords" (e.g., "Remote", "$200k+", "Research Scientist").

### Key Entities

- **Job**: `{ id, title, company, location, salary, description, url, date_posted, source_site }`
- **Intelligence**: `{ job_id, priority_score, match_explanation, auto_tags }`
- **Config**: `{ keywords, negative_keywords, min_salary, locations }`

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Scraper fetches at least 50 relevant jobs in under 2 minutes.
- **SC-002**: Dashboard loads in < 500ms (Localhost).
- **SC-003**: "Zombie Killing" script successfully frees port 7777 100% of the time on startup.
- **SC-004**: LLM analysis adds "Intelligence" fields to 100% of scraped jobs.
- **SC-005**: Zero "manual intervention" required for daily updates (fully automated script).
